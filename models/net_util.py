from .arches import *from .layers import SNNConvLayerfrom .layers import APLayerfrom .layers import SeqToANNContainerfrom .convlstm import ConvLSTMimport numpy as npclass ChannelAttention(nn.Module):    def __init__(self, in_planes, ratio=16):        super(ChannelAttention, self).__init__()        self.avg_pool = nn.AdaptiveAvgPool2d(1)        self.max_pool = nn.AdaptiveMaxPool2d(1)        self.fc1 = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)        self.relu1 = nn.ReLU(inplace=True)        self.fc2 = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)        self.sigmoid = nn.Sigmoid()    def forward(self, x):        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))        out = avg_out + max_out        out=self.sigmoid(out)        return outclass ChannelAttention_softmax(nn.Module):    def __init__(self, in_planes, ratio=16,L=32,M=2):        super(ChannelAttention_softmax, self).__init__()        self.avg_pool = nn.AdaptiveAvgPool2d(1)        self.max_pool = nn.AdaptiveMaxPool2d(1)        self.in_planes=in_planes        self.M=M        d = max(in_planes // ratio, L)        self.fc1=nn.Sequential(nn.Conv2d(in_planes,d,1,bias=False),                               nn.ReLU(inplace=True))        self.fc2=nn.Conv2d(d,in_planes*2,1,1,bias=False)        self.softmax=nn.Softmax(dim=1)        self.sigmoid = nn.Sigmoid()    def forward(self, x):        avg_out=self.avg_pool(x)        max_out=self.max_pool(x)        out = avg_out + max_out        out = self.fc1(out)        out_two = self.fc2(out)        batch_size = x.size(0)        out_two=out_two.reshape(batch_size,self.M,self.in_planes,-1)        out_two = self.softmax(out_two)        x_i, x_e = out_two[:, 0:1, :, :], out_two[:, 1:2, :, :]        x_i = x_i.reshape(batch_size, self.in_planes, 1, 1)        x_e = x_e.reshape(batch_size, self.in_planes, 1, 1)        return x_i, x_eclass SpatialAttention(nn.Module):    def __init__(self):        super(SpatialAttention, self).__init__()        self.conv1 = nn.Conv2d(2, 1,kernel_size=(3,3), padding=(1,1), bias=False)        self.sigmoid = nn.Sigmoid()    def forward(self, x):        avg_out = torch.mean(x, dim=1, keepdim=True)        max_out, _ = torch.max(x, dim=1, keepdim=True)        x = torch.cat([avg_out, max_out], dim=1)        x = self.conv1(x)        x = self.sigmoid(x)        return xclass SpatialAttention_softmax(nn.Module):    def __init__(self):        super(SpatialAttention_softmax, self).__init__()        self.conv1 = nn.Conv2d(2, 1,kernel_size=(3,3), padding=(1,1), bias=False)        self.conv2 = nn.Conv2d(2, 1,kernel_size=(3,3), padding=(1,1), bias=False)        self.softmax = nn.Softmax(dim=1)        self.sigmoid = nn.Sigmoid()    def forward(self, x):        avg_out = torch.mean(x, dim=1, keepdim=True)        max_out, _ = torch.max(x, dim=1, keepdim=True)        x = torch.cat([avg_out, max_out], dim=1)        x_i = self.conv1(x)        x_e = self.conv2(x)        x = torch.cat([x_i, x_e], dim=1)        x = self.softmax(x)        x_i, x_e = x[:, 0:1, :, :], x[:, 1:2, :, :]        return x_i,x_eclass CNNSpatialAttention(nn.Module):    def __init__(self, groups):        super(CNNSpatialAttention, self).__init__()        self.groups = groups        self.avg_pool = nn.AdaptiveAvgPool2d(1)        self.weight = nn.Parameter(torch.zeros(1, groups, 1, 1))        self.bias = nn.Parameter(torch.zeros(1, groups, 1, 1))        self.sig = nn.Sigmoid()        self.init_weights()    def init_weights(self):        for m in self.modules():            if isinstance(m, nn.Conv2d):                init.kaiming_normal_(m.weight, mode='fan_out')                if m.bias is not None:                    init.constant_(m.bias, 0)            elif isinstance(m, nn.BatchNorm2d):                init.constant_(m.weight, 1)                init.constant_(m.bias, 0)            elif isinstance(m, nn.Linear):                init.normal_(m.weight, std=0.001)                if m.bias is not None:                    init.constant_(m.bias, 0)    def forward(self, x):        b, c, h, w = x.shape        x = x.view(b * self.groups, -1, h, w)  # bs*g,dim//g,h,w        xn = x * self.avg_pool(x)  # bs*g,dim//g,h,w        xn = xn.sum(dim=1, keepdim=True)  # bs*g,1,h,w        t = xn.view(b * self.groups, -1)  # bs*g,h*w        t = t - t.mean(dim=1, keepdim=True)  # bs*g,h*w        std = t.std(dim=1, keepdim=True) + 1e-5        t = t / std  # bs*g,h*w        t = t.view(b, self.groups, h, w)  # bs,g,h*w        t = t * self.weight + self.bias  # bs,g,h*w        t = t.view(b * self.groups, 1, h, w)  # bs*g,1,h*w        x = x * self.sig(t)        x = x.view(b, c, h, w)        return x# Channel Attention Layerclass CALayer(nn.Module):    def __init__(self, channel, reduction=16, bias=False):        super(CALayer, self).__init__()        # global average pooling: feature --> point        self.avg_pool = nn.AdaptiveAvgPool2d(1)        # feature channel downscale and upscale --> channel weight        self.conv_du = nn.Sequential(                nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=bias),                nn.ReLU(inplace=True),                nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=bias),                nn.Sigmoid()        )    def forward(self, x):        y = self.avg_pool(x)        y = self.conv_du(y)        return x * y## Channel Attention Block (CAB)class CAB(nn.Module):    def __init__(self, n_feat, kernel_size, reduction, bias, act):        super(CAB, self).__init__()        modules_body = []        modules_body.append(conv(n_feat, n_feat, kernel_size, bias=bias))        modules_body.append(act)        modules_body.append(conv(n_feat, n_feat, kernel_size, bias=bias))        self.CA = CALayer(n_feat, reduction, bias=bias)        self.body = nn.Sequential(*modules_body)    def forward(self, x):        res = self.body(x)        res = self.CA(res)        res += x        return resclass shallow_cell(nn.Module):    def __init__(self,inChannels, outChannels):        super(shallow_cell, self).__init__()        self.n_feats = outChannels        act = nn.ReLU(inplace=True)        bias = False        reduction = 4        self.shallow_feat = nn.Sequential(conv(inChannels, self.n_feats, 3, bias=bias),                                           CAB(self.n_feats, 3, reduction, bias=bias, act=act))    def forward(self,x):        feat = self.shallow_feat(x)        return feat# 用于ResNet18和34的残差块，用的是2个3x3的卷积class EN_CNN_Block(nn.Module):    def __init__(self, in_planes, planes, kernel_size=3, reduction=4, bias=False):        super(EN_CNN_Block, self).__init__()        act = nn.ReLU(inplace=True)        self.down = DownSample(in_planes, planes)        self.encoder = [CAB(planes, kernel_size, reduction, bias=bias, act=act) for _ in range(2)]        self.encoder = nn.Sequential(*self.encoder)    def forward(self, x):        x = self.down(x)        x=self.encoder(x)        return xclass EN_SNN_Block(nn.Module):    def __init__(self, in_planes, planes, kernel_size=3):        super(EN_SNN_Block, self).__init__()        self.conv = SNNConvLayer(in_planes, planes, kernel_size, stride= 1, padding=1)        self.pool = APLayer(2)        self.snntatt = SNNTem_Module()        self.spaatt = CNNSpatialAttention(int(planes / 8))    def forward(self, s):        s1 = self.conv(s)        s2 = self.pool(s1)        s3, attmap = self.snntatt(s2)        mapeds = s3.sum(dim=1)        mapeds = self.spaatt(mapeds)        return mapeds, s2class EN_base_SNN_Block(nn.Module):    def __init__(self, in_planes, planes, kernel_size=3):        super(EN_base_SNN_Block, self).__init__()        self.conv = SNNConvLayer(in_planes, planes, kernel_size, stride= 1, padding=1)        self.pool = APLayer(2)    def forward(self, s):        s1 = self.conv(s)        s2 = self.pool(s1)        return s2class EN_SNN_TemATT_Block(nn.Module):    def __init__(self, in_planes, planes, kernel_size=3):        super(EN_SNN_TemATT_Block, self).__init__()        self.conv = SNNConvLayer(in_planes, planes, kernel_size, stride= 1, padding=1)        self.pool = APLayer(2)        self.snntatt = SNNTem_Module()    def forward(self, s):        s1 = self.conv(s)        s2 = self.pool(s1)        s3, attmap = self.snntatt(s2)        return s3class EN_SNN_SpaATT_Block(nn.Module):    def __init__(self, in_planes, planes, kernel_size=3):        super(EN_SNN_SpaATT_Block, self).__init__()        self.conv = SNNConvLayer(in_planes, planes, kernel_size, stride= 1, padding=1)        self.pool = APLayer(2)        self.spaatt = CNNSpatialAttention(int(planes / 8))    def forward(self, s):        s1 = self.conv(s)        s2 = self.pool(s1)        attmap = self.spaatt(s2.sum(dim=1))        return attmap, s2class TSST_EN_SNN_Block(nn.Module):    def __init__(self, in_planes, planes, scale=4, kernel_size=3):        super(TSST_EN_SNN_Block, self).__init__()        self.conv = SNNConvLayer(in_planes, planes, kernel_size, stride= 1, padding=1)        self.pool = APLayer(2)        # self.snntatt = SNNTem_Module()        # self.spaatt = CNNSpatialAttention(int(planes / 8))        self.tsst = TSST(d_model=planes, d_k=planes, d_v=planes, scale=scale)    def forward(self, s):        s1 = self.conv(s)        s2 = self.pool(s1)        self.tsst(s2)  # 2, 10, 4, 512, 512        mapeds = self.tsst.fusionattention(s2)        # s3, attmap = self.snntatt(s2)        # mapeds = s3.sum(dim=1)        # mapeds = self.spaatt(mapeds)        return s2, mapedsclass EN_LSTM_Block(nn.Module):    def __init__(self, in_planes, planes, kernel_size=3):        super(EN_LSTM_Block, self).__init__()        self.convlstm = ConvLSTM(input_dim=in_planes, hidden_dim=planes, kernel_size=(kernel_size, kernel_size), num_layers=1,                            batch_first=True)        self.pool = SeqToANNContainer(nn.AvgPool2d(2))    def forward(self, s):        s1, _ = self.convlstm(s)        s2 = self.pool(s1[0])        return s2, s2class SNNTem_Module(nn.Module):    """ Temporal-Attention (TA) Module """    def __init__(self):        super(SNNTem_Module, self).__init__()        # self.chanel_in = in_dim        self.gamma = nn.Parameter(torch.zeros(1))        self.softmax  = nn.Softmax(dim=-1)    def forward(self,x):        """            inputs :                x : input feature maps( B X T X C X H X W)            returns :                out : attention value + input feature                attention: B X C X C        """        m_batchsize, Ts, C, height, width = x.size()  # [12, 10, 2, 512, 512]        proj_query = x.view(m_batchsize, Ts, -1)        proj_key = x.view(m_batchsize, Ts, -1).permute(0, 2, 1)        energy = torch.bmm(proj_query, proj_key)        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy) - energy        attention = self.softmax(energy_new)        proj_value = x.view(m_batchsize, Ts, -1)        out = torch.bmm(attention, proj_value)        out = out.view(m_batchsize, Ts, C, height, width)        out = self.gamma * out + x        return out, attentionclass DE_Block(nn.Module):    def __init__(self, in_planes, planes, kernel_size=3, reduction=4, bias=False):        super(DE_Block, self).__init__()        act = nn.ReLU(inplace=True)        self.up=SkipUpSample(in_planes, planes)        self.decoder = [CAB(planes, kernel_size, reduction, bias=bias, act=act) for _ in range(2)]        self.decoder = nn.Sequential(*self.decoder)        self.skip_attn = CAB(planes, kernel_size, reduction, bias=bias, act=act)    def forward(self, x, skpCn):        x = self.up(x, self.skip_attn(skpCn))        x = self.decoder(x)        return xclass TSST(torch.nn.Module):    def __init__(self, d_model, d_k, d_v, scale=4):        super(TSST, self).__init__()        self.fc_q = nn.Linear(d_model, d_k)        self.fc_k = nn.Linear(d_model, d_k)        # self.fc_v = nn.Linear(d_model, d_v)        self.fc_o = nn.Linear(d_v, d_model)        # self.dropout = nn.Dropout(dropout)        self.d_model = d_model        self.d_k = d_k        self.d_v = d_v        # self.h = h        # self.init_weights()        self.ATT = None        self.raw_size = None        self.down_size = None        self.scale = scale    def init_weights(self):        for m in self.modules():            if isinstance(m, nn.Conv2d):                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out')                if m.bias is not None:                    torch.nn.init.constant_(m.bias, 0)            elif isinstance(m, nn.BatchNorm2d):                torch.nn.init.constant_(m.weight, 1)                torch.nn.init.constant_(m.bias, 0)            elif isinstance(m, nn.Linear):                torch.nn.init.normal_(m.weight, std=0.001)                if m.bias is not None:                    torch.nn.init.constant_(m.bias, 0)    def downsample(self, data, scale):        N, ts, c, x, y= data.shape        if not x % scale == 0:            zero = torch.zeros([N, ts, c, scale - x%scale,y], dtype=data.dtype, device=data.device)            data = torch.cat((data, zero), dim=3)            x = data.shape[2]        if not y % scale == 0:            zero = torch.zeros([N, ts, c, x, scale - y%scale], dtype=data.dtype, device=data.device)            data = torch.cat((data, zero), dim=4)            y = data.shape[3]        # dsspike = data.view(N, c, round(data.shape[2] / scale), scale, round(data.shape[3] / scale), scale, ts).sum(dim=[3, 5])        dsspike = data.view(N, ts, c, round(data.shape[3] / scale), scale, round(data.shape[4] / scale), scale).sum(dim=[4, 6])        return dsspike    def upsample(self, data, shape, scale):        N,x,y,t,t = data.shape        data = data.unsqueeze(3).unsqueeze(2)        data = data.expand(N,x,scale,y,scale,t,t).contiguous().view(N,x*scale,y*scale,t,t)        data = data[:,:shape[0],:shape[1],:,:]        return data    def forward(self, inputspike):        '''            inputs :                x : input feature maps( B X T X C X H X W)            Computes            :param queries: Queries (b_s, nq, d_model)            :param keys: Keys (b_s, nk, d_model)            :param values: Values (b_s, nk, d_model)            :param attention_mask: Mask over attention values (b_s, h, nq, nk). True indicates masking.            :param attention_weights: Multiplicative weights for attention values (b_s, h, nq, nk).            returns :                out : attention value + input feature                attention: B X C X C        '''        spike = inputspike        self.raw_size = spike.shape        # N, c, x, y, ts = self.raw_size        dsspike = self.downsample(spike, self.scale)        self.down_size = dsspike.shape        dsN, dsts, dsc, dsx, dsy  = self.down_size        queries = dsspike.permute(0, 3, 4, 1, 2).contiguous().view(dsN * dsx * dsy, dsts, -1)  # (N*dsx*dsy, dsts, dsc)        keys = dsspike.permute(0, 3, 4, 1, 2).contiguous().view(dsN * dsx * dsy, dsts, -1)     # (N*dsx*dsy, dsts, dsc)        b_s, nq = queries.shape[:2]        nk = keys.shape[1]        q = self.fc_q(queries).view(b_s, nq, self.d_k).permute(0, 1, 2)         # (b_s, nq, d_k)        k = self.fc_k(keys).view(b_s, nk, self.d_k).permute(0, 2, 1)            # (b_s, d_k, nk)        # v = self.fc_v(values).view(b_s, nk, self.d_v).permute(0, 2, 1, 3)     # (b_s, h, nk, d_v)        att = torch.matmul(q, k) / np.sqrt(self.d_k)                            # (N*dsx*dsy, dsts, dsts)        att = torch.softmax(att, -1)        self.ATT = att        return self.ATT    def fireattention(self, psp):        '''        get scaled psp from temporal attention map.        Args:            psp: input psp tensor, psp[N, c, x, y, ts]        Returns:        '''        N, c, x, y, ts = psp.shape        dsN, dsc, dsx, dsy, dsts = self.down_size        if not x % self.scale == 0:            zeros = torch.zeros(N, c, self.scale-x%self.scale, y, ts,dtype=psp.dtype,device=psp.device)            psp = torch.cat((psp, zeros), dim=2)        if not y % self.scale == 0:            zeros = torch.zeros(N, c, psp.shape[2], self.scale-y%self.scale,ts,dtype=psp.dtype,device=psp.device)            psp = torch.cat((psp, zeros), dim=3)        for i in range(self.scale):            for j in range(self.scale):                localpsp = psp[:,:,i::self.scale,j::self.scale,:].permute(0,2,3,4,1).contiguous().view(-1, ts, c)                localpsp = torch.matmul(self.ATT, localpsp).view(N, dsx, dsy, ts, c).permute(0, 4, 1, 2, 3)                psp[:,:,i::self.scale,j::self.scale,:] = localpsp        psp = psp[:,:,:x,:y,:]        return psp    def fusionattention(self, spike):        N, ts, c, x, y = spike.shape        dsN, dsts, dsc, dsx, dsy = self.down_size        # if not x % self.scale == 0:        #     zeros = torch.zeros(N, c, self.scale-x%self.scale, y, ts,dtype=spike.dtype,device=spike.device)        #     spike = torch.cat((spike, zeros), dim=2)        # if not y % self.scale == 0:        #     zeros = torch.zeros(N, c, spike.shape[2], self.scale-y%self.scale,ts,dtype=spike.dtype,device=spike.device)        #     spike = torch.cat((spike, zeros), dim=3)        full_attmap = self.upsample(self.ATT.view(dsN, dsx, dsy, dsts, dsts), [x, y], self.scale)        # full_attmap = full_attmap[:, :x, :y, :, :]        spike1 = spike.permute(0, 3, 4, 1, 2).contiguous().view(-1, ts, c)        v = self.fc_o(spike1)        out = torch.matmul(full_attmap.contiguous().view(N*x*y, ts, ts), v).contiguous().view(N, x, y, ts, c)        out = out.permute(0, 3, 4, 1, 2)        # for i in range(self.scale):        #     for j in range(self.scale):        #         localpsp = spike[:,:,i::self.scale,j::self.scale,:].permute(0,2,3,4,1).contiguous().view(-1, ts, c)        #         localpsp = torch.matmul(self.ATT, localpsp).view(N, dsx, dsy, ts, c).permute(0, 4, 1, 2, 3)        #         spike[:,:,i::self.scale,j::self.scale,:] = localpsp        # spike = spike[:,:,:x,:y,:]        return out# class TSST(torch.nn.Module):#     def __init__(self, d_model, d_k, d_v, scale=4):#         super(TSST, self).__init__()#         self.fc_q = nn.Linear(d_model, d_k)#         self.fc_k = nn.Linear(d_model, d_k)#         # self.fc_v = nn.Linear(d_model, d_v)#         self.fc_o = nn.Linear(d_v, d_model)#         # self.dropout = nn.Dropout(dropout)#         self.d_model = d_model#         self.d_k = d_k#         # self.d_v = d_v#         # self.h = h#         # self.init_weights()#         self.ATT = None#         self.raw_size = None#         self.down_size = None#         self.scale = scale##     def init_weights(self):#         for m in self.modules():#             if isinstance(m, nn.Conv2d):#                 torch.nn.init.kaiming_normal_(m.weight, mode='fan_out')#                 if m.bias is not None:#                     torch.nn.init.constant_(m.bias, 0)#             elif isinstance(m, nn.BatchNorm2d):#                 torch.nn.init.constant_(m.weight, 1)#                 torch.nn.init.constant_(m.bias, 0)#             elif isinstance(m, nn.Linear):#                 torch.nn.init.normal_(m.weight, std=0.001)#                 if m.bias is not None:#                     torch.nn.init.constant_(m.bias, 0)##     def downsample(self, data, scale):#         N, ts, c, x, y= data.shape#         if not x % scale == 0:#             zero = torch.zeros([N, ts, c, scale - x%scale,y], dtype=data.dtype, device=data.device)#             data = torch.cat((data, zero), dim=2)#             x = data.shape[2]#         if not y % scale == 0:#             zero = torch.zeros([N, ts, c, x, scale - y%scale], dtype=data.dtype, device=data.device)#             data = torch.cat((data, zero), dim=3)#             y = data.shape[3]#         # dsspike = data.view(N, c, round(data.shape[2] / scale), scale, round(data.shape[3] / scale), scale, ts).sum(dim=[3, 5])#         dsspike = data.view(N, ts, c, round(data.shape[3] / scale), scale, round(data.shape[4] / scale), scale).sum(dim=[4, 6])#         return dsspike##     def upsample(self, data, shape, scale):#         N,x,y,t,t = data.shape#         data = data.unsqueeze(3).unsqueeze(2)#         data = data.expand(N,x,scale,y,scale,t,t).contiguous().view(N,x*scale,y*scale,t,t)#         data = data[:,:shape[0],:shape[1],:,:]##         return data##     def forward(self, inputspike):#         '''#             inputs :#                 x : input feature maps( B X T X C X H X W)#             Computes#             :param queries: Queries (b_s, nq, d_model)#             :param keys: Keys (b_s, nk, d_model)#             :param values: Values (b_s, nk, d_model)#             :param attention_mask: Mask over attention values (b_s, h, nq, nk). True indicates masking.#             :param attention_weights: Multiplicative weights for attention values (b_s, h, nq, nk).#             returns :#                 out : attention value + input feature#                 attention: B X C X C#         '''#         spike = inputspike#         self.raw_size = spike.shape#         # N, c, x, y, ts = self.raw_size#         dsspike = self.downsample(spike, self.scale)#         self.down_size = dsspike.shape#         dsN, dsts, dsc, dsx, dsy  = self.down_size##         queries = dsspike.permute(0, 3, 4, 1, 2).contiguous().view(dsN * dsx * dsy, dsts, -1)  # (N*dsx*dsy, dsts, dsc)#         keys = dsspike.permute(0, 3, 4, 1, 2).contiguous().view(dsN * dsx * dsy, dsts, -1)     # (N*dsx*dsy, dsts, dsc)##         b_s, nq = queries.shape[:2]#         nk = keys.shape[1]##         q = self.fc_q(queries).view(b_s, nq, self.d_k).permute(0, 1, 2)         # (b_s, nq, d_k)#         k = self.fc_k(keys).view(b_s, nk, self.d_k).permute(0, 2, 1)            # (b_s, d_k, nk)#         # v = self.fc_v(values).view(b_s, nk, self.d_v).permute(0, 2, 1, 3)     # (b_s, h, nk, d_v)##         att = torch.matmul(q, k) / np.sqrt(self.d_k)                            # (N*dsx*dsy, dsts, dsts)#         att = torch.softmax(att, -1)#         self.ATT = att##         return self.ATT##     def fireattention(self, psp):#         '''#         get scaled psp from temporal attention map.#         Args:#             psp: input psp tensor, psp[N, c, x, y, ts]#         Returns:#         '''#         N, c, x, y, ts = psp.shape#         dsN, dsc, dsx, dsy, dsts = self.down_size#         if not x % self.scale == 0:#             zeros = torch.zeros(N, c, self.scale-x%self.scale, y, ts,dtype=psp.dtype,device=psp.device)#             psp = torch.cat((psp, zeros), dim=2)#         if not y % self.scale == 0:#             zeros = torch.zeros(N, c, psp.shape[2], self.scale-y%self.scale,ts,dtype=psp.dtype,device=psp.device)#             psp = torch.cat((psp, zeros), dim=3)##         for i in range(self.scale):#             for j in range(self.scale):#                 localpsp = psp[:,:,i::self.scale,j::self.scale,:].permute(0,2,3,4,1).contiguous().view(-1, ts, c)#                 localpsp = torch.matmul(self.ATT, localpsp).view(N, dsx, dsy, ts, c).permute(0, 4, 1, 2, 3)#                 psp[:,:,i::self.scale,j::self.scale,:] = localpsp##         psp = psp[:,:,:x,:y,:]##         return psp##     def fusionattention(self, spike):#         N, ts, c, x, y = spike.shape#         dsN, dsts, dsc, dsx, dsy = self.down_size#         if not x % self.scale == 0:#             zeros = torch.zeros(N, c, self.scale-x%self.scale, y, ts,dtype=spike.dtype,device=spike.device)#             spike = torch.cat((spike, zeros), dim=2)#         if not y % self.scale == 0:#             zeros = torch.zeros(N, c, spike.shape[2], self.scale-y%self.scale,ts,dtype=spike.dtype,device=spike.device)#             spike = torch.cat((spike, zeros), dim=3)##         for i in range(self.scale):#             for j in range(self.scale):#                 localpsp = spike[:,:,i::self.scale,j::self.scale,:].permute(0,2,3,4,1).contiguous().view(-1, ts, c)#                 localpsp = torch.matmul(self.ATT, localpsp).view(N, dsx, dsy, ts, c).permute(0, 4, 1, 2, 3)#                 spike[:,:,i::self.scale,j::self.scale,:] = localpsp##         spike = spike[:,:,:x,:y,:]##         return spikeclass Self_Attention(nn.Module):    def __init__(self,in_planes):        super(Self_Attention, self).__init__()        self.conv1 = nn.Conv2d(2, 1,kernel_size=(1,1), bias=False)        self.conv2 = nn.Conv2d(in_planes, in_planes,kernel_size=(2,1),  bias=False)        self.conv3 = nn.Conv2d(in_planes, in_planes,kernel_size=(1,2), bias=False)        self.sigmoid = nn.Sigmoid()    def forward(self, x):        avg_out_1 = torch.mean(x, dim=1, keepdim=True)        max_out_1, _ = torch.max(x, dim=1, keepdim=True)        x_1 = torch.cat([avg_out_1, max_out_1], dim=1)        x_1 = self.conv1(x_1)        x_1 = self.sigmoid(x_1)        avg_out_2 = torch.mean(x, dim=2, keepdim=True)        max_out_2, _ = torch.max(x, dim=2, keepdim=True)        x_2 = torch.cat([avg_out_2, max_out_2], dim=2)        x_2 = self.conv2(x_2)        x_2 = self.sigmoid(x_2)        avg_out_3 = torch.mean(x, dim=3, keepdim=True)        max_out_3, _ = torch.max(x, dim=3, keepdim=True)        x_3 = torch.cat([avg_out_3, max_out_3], dim=3)        x_3 = self.conv3(x_3)        x_3 = self.sigmoid(x_3)        att_weight=x_1*x_2*x_3        enhance_feature=x*att_weight        return enhance_feature# class Self_Attention(nn.Module):##     def __init__(self, in_planes):#         super(Self_Attention, self).__init__()###         self.conv1 = nn.Conv2d(2, 1,kernel_size=(3,3), padding=(1,1), bias=False)#         self.sigmoid = nn.Sigmoid()#####     def forward(self, common, differ):#         batch_size, channels, height, width = common.shape#         common_q = self.query_common(common).view(batch_size, -1, height * width).permute(0, 2, 1)#         common_k = self.key_common(common).view(batch_size, -1, height * width)#         common_v = self.value_common(common).view(batch_size, -1, height * width)#         print(common_k.size())#         common_attn_matrix = torch.bmm(common_q, common_k)  # torch.bmm进行tensor矩阵乘法,q与k相乘得到的值为attn_matrix.#         common_attn_matrix = self.softmax(common_attn_matrix)  # 经过一个softmax进行缩放权重大小.#         out_common = torch.bmm(common_v, common_attn_matrix.permute(0, 2, 1))  # tensor.permute将矩阵的指定维进行换位.这里将1于2进行换位。#         out_common = out_common.view(*common.shape)#         enhance_commom=self.gamma_common * out_common + common##         batch_size, channels, height, width = differ.shape#         differ_q = self.query_differ(differ).view(batch_size, -1, height * width).permute(0, 2, 1)#         differ_k = self.key_differ(differ).view(batch_size, -1, height * width)#         differ_v = self.value_differ(differ).view(batch_size, -1, height * width)#         differ_attn_matrix = torch.bmm(differ_q, differ_k)  # torch.bmm进行tensor矩阵乘法,q与k相乘得到的值为attn_matrix.#         differ_attn_matrix = self.softmax(differ_attn_matrix)  # 经过一个softmax进行缩放权重大小.#         out_differ = torch.bmm(differ_v, differ_attn_matrix.permute(0, 2, 1))  # tensor.permute将矩阵的指定维进行换位.这里将1于2进行换位。#         out_differ= out_differ.view(*differ.shape)#         enhance_differ=self.gamma_differ * out_differ + differ####         return enhance_commom, enhance_differ